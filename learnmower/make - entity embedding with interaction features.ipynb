{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# feature libraries\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# timer function\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/kaggle_data'\n",
    "feat_dir = '../input/features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load data:] done in 5 s\n",
      "[Loading Label Encoded Features:] done in 3 s\n",
      "[Loading Numeric Encoded Features:] done in 3 s\n",
      "[Loading NIMA Features:] done in 4 s\n",
      "[Loading Image Features:] done in 17 s\n"
     ]
    }
   ],
   "source": [
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "with timer(\"load data:\"):\n",
    "    usecols = ['item_id']\n",
    "    train = pd.read_csv(f'{data_dir}/train.csv', index_col=\"item_id\", usecols=usecols+['deal_probability'])\n",
    "    test = pd.read_csv(f'{data_dir}/test.csv', index_col=\"item_id\", usecols=usecols)\n",
    "\n",
    "    train_split = len(train)\n",
    "    y = train['deal_probability'].copy()\n",
    "    train.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Loading Label Encoded Features:\"):\n",
    "    train_le = pd.read_csv(f'{feat_dir}/train_le.csv', index_col=\"item_id\")\n",
    "    test_le = pd.read_csv(f'{feat_dir}/test_le.csv', index_col=\"item_id\")\n",
    "    \n",
    "    train = train.merge(train_le, how='left', left_index=True, right_index=True)\n",
    "    test = test.merge(test_le, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    cat_cols += ['region','city','parent_category_name','category_name','user_type','param_1','param_2','param_3']\n",
    "    \n",
    "    del train_le, test_le\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Loading Numeric Encoded Features:\"):\n",
    "    train_numeric = pd.read_csv(f'{feat_dir}/train_numeric.csv', index_col=\"item_id\")\n",
    "    test_numeric = pd.read_csv(f'{feat_dir}/test_numeric.csv', index_col=\"item_id\")\n",
    "    \n",
    "    train = train.merge(train_numeric, how='left', left_index=True, right_index=True)\n",
    "    test = test.merge(test_numeric, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    cat_cols += ['price_missing','image_top_1_missing']\n",
    "    num_cols += ['item_seq_number','image_top_1','price']\n",
    "\n",
    "    del train_numeric, test_numeric\n",
    "    gc.collect()\n",
    "    \n",
    "with timer(\"Loading NIMA Features:\"):\n",
    "    train_nima = pd.read_csv(f'{feat_dir}/train_nima.csv', index_col=\"item_id\")\n",
    "    test_nima = pd.read_csv(f'{feat_dir}/test_nima.csv', index_col=\"item_id\")\n",
    "    \n",
    "    train = train.merge(train_nima, how='left', left_index=True, right_index=True)\n",
    "    test = test.merge(test_nima, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    num_cols += [\"mobile_mean\", \"mobile_std\",\"inception_mean\", \"inception_std\", \"nasnet_mean\", \"nasnet_std\"]\n",
    "\n",
    "    del train_nima, test_nima\n",
    "    gc.collect() \n",
    "    \n",
    "with timer(\"Loading Image Features:\"):\n",
    "    train_img = pd.read_csv(f'{feat_dir}/train_img.csv', index_col=\"item_id\")\n",
    "    test_img = pd.read_csv(f'{feat_dir}/test_img.csv', index_col=\"item_id\")\n",
    "    \n",
    "    train = train.merge(train_img, how='left', left_index=True, right_index=True)\n",
    "    test = test.merge(test_img, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    num_cols += ['img_size_x','img_size_y','img_file_size','img_mean_color','thing1','thing2',\n",
    "                 'img_sobel00','img_sobel10','img_sobel20','img_sobel01','img_sobel11','img_sobel21',\n",
    "                 'img_kurtosis','img_skew','img_dullness_light_percent','img_dullness_dark_percent','img_blur',\n",
    "                 'img_blue_mean','img_green_mean','img_red_mean','img_blue_std','img_green_std','img_red_std',\n",
    "                 'img_average_red','img_average_green','img_average_blue']\n",
    "\n",
    "    del train_img, test_img\n",
    "    gc.collect()\n",
    "    \n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.4, random_state=0)\n",
    "\n",
    "mdl = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=0)\n",
    "mdl.fit(X_train, y_train)\n",
    "\n",
    "for name, importance in zip(list(X_train), mdl.feature_importances_):\n",
    "    print(name, \"=\", importance)\n",
    "\n",
    "del X_train, X_test, y_train, y_test \n",
    "gc.collect()\n",
    "\n",
    "# region = 0.05078480904906774 \n",
    "# city = 0.0669028031982457 ###\n",
    "# parent_category_name = 0.01892913897631363\n",
    "# category_name = 0.018579901012128967\n",
    "# param_1 = 0.049909490828195055\n",
    "# param_2 = 0.03443197725200006\n",
    "# param_3 = 0.013801396359618488\n",
    "# user_type = 0.009652674217647102\n",
    "# item_seq_number = 0.09532802580719614 ###\n",
    "# image_top_1 = 0.11236131987282504 ###\n",
    "# price_missing = 0.003581317998232572\n",
    "# image_top_1_missing = 0.016743076249108438\n",
    "# price = 0.07502286721506463 ###\n",
    "# mobile_mean = 0.07310568363879322 ###\n",
    "# mobile_std = 0.07459486368530469 ###\n",
    "# inception_mean = 0.06958129445962614 ###\n",
    "# inception_std = 0.0731361226167795 ###\n",
    "# nasnet_mean = 0.06996479172642771 ###\n",
    "# nasnet_std = 0.07358844583742533 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Create potential interactions:] done in 2 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Create potential interactions:\"):\n",
    "    \n",
    "    important_cols = ['image_top_1','price', 'city', 'item_seq_number', 'mobile_mean', 'mobile_std', 'inception_mean',\n",
    "                      'inception_std', 'nasnet_mean', 'nasnet_std']\n",
    "    \n",
    "    def interaction_features(train, test, fea1, fea2, prefix):\n",
    "        train['inter_{}*'.format(prefix)] = train[fea1] * train[fea2]\n",
    "        train['inter_{}/'.format(prefix)] = train[fea1] / train[fea2]\n",
    "\n",
    "        test['inter_{}*'.format(prefix)] = test[fea1] * test[fea2]\n",
    "        test['inter_{}/'.format(prefix)] = test[fea1] / test[fea2]\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    for e, (x, y) in enumerate(combinations(important_cols, 2)):\n",
    "        train, test = interaction_features(train, test, x, y, e)\n",
    "        \n",
    "    inter_cols = [x for x in list(train) if 'inter' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subset numeric features:] done in 2 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Subset numeric features:\"):\n",
    "    train_num = train[[x for x in list(train) if x in num_cols]]\n",
    "    test_num = test[[x for x in list(test) if x in num_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>price</th>\n",
       "      <th>mobile_mean</th>\n",
       "      <th>mobile_std</th>\n",
       "      <th>inception_mean</th>\n",
       "      <th>inception_std</th>\n",
       "      <th>nasnet_mean</th>\n",
       "      <th>nasnet_std</th>\n",
       "      <th>img_size_x</th>\n",
       "      <th>...</th>\n",
       "      <th>img_blur</th>\n",
       "      <th>img_blue_mean</th>\n",
       "      <th>img_green_mean</th>\n",
       "      <th>img_red_mean</th>\n",
       "      <th>img_blue_std</th>\n",
       "      <th>img_green_std</th>\n",
       "      <th>img_red_std</th>\n",
       "      <th>img_average_red</th>\n",
       "      <th>img_average_green</th>\n",
       "      <th>img_average_blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.136324</td>\n",
       "      <td>-0.152807</td>\n",
       "      <td>-0.005147</td>\n",
       "      <td>0.376462</td>\n",
       "      <td>0.196650</td>\n",
       "      <td>0.038229</td>\n",
       "      <td>0.360435</td>\n",
       "      <td>-0.071322</td>\n",
       "      <td>0.059919</td>\n",
       "      <td>-0.151260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345490</td>\n",
       "      <td>-0.280614</td>\n",
       "      <td>-0.638004</td>\n",
       "      <td>0.687033</td>\n",
       "      <td>0.641418</td>\n",
       "      <td>0.851439</td>\n",
       "      <td>0.132975</td>\n",
       "      <td>-0.280614</td>\n",
       "      <td>-0.638004</td>\n",
       "      <td>0.687033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.133284</td>\n",
       "      <td>-0.472375</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>0.380049</td>\n",
       "      <td>0.214786</td>\n",
       "      <td>0.588905</td>\n",
       "      <td>0.167156</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>-0.033727</td>\n",
       "      <td>-0.138074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275917</td>\n",
       "      <td>-0.608886</td>\n",
       "      <td>-0.394327</td>\n",
       "      <td>-0.266605</td>\n",
       "      <td>0.451465</td>\n",
       "      <td>0.240082</td>\n",
       "      <td>0.133307</td>\n",
       "      <td>-0.608886</td>\n",
       "      <td>-0.394327</td>\n",
       "      <td>-0.266605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.135073</td>\n",
       "      <td>1.894048</td>\n",
       "      <td>-0.005083</td>\n",
       "      <td>0.542563</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.241327</td>\n",
       "      <td>0.945804</td>\n",
       "      <td>0.401474</td>\n",
       "      <td>0.072903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254716</td>\n",
       "      <td>1.487169</td>\n",
       "      <td>1.379419</td>\n",
       "      <td>1.140900</td>\n",
       "      <td>1.906005</td>\n",
       "      <td>1.904254</td>\n",
       "      <td>1.820077</td>\n",
       "      <td>1.487169</td>\n",
       "      <td>1.379419</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085533</td>\n",
       "      <td>-0.367201</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>0.440850</td>\n",
       "      <td>0.415528</td>\n",
       "      <td>0.831658</td>\n",
       "      <td>0.253381</td>\n",
       "      <td>0.486166</td>\n",
       "      <td>0.321294</td>\n",
       "      <td>-0.138074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372882</td>\n",
       "      <td>2.253272</td>\n",
       "      <td>2.131517</td>\n",
       "      <td>1.853883</td>\n",
       "      <td>0.683302</td>\n",
       "      <td>0.762878</td>\n",
       "      <td>0.720346</td>\n",
       "      <td>2.253272</td>\n",
       "      <td>2.131517</td>\n",
       "      <td>1.853883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.136146</td>\n",
       "      <td>1.117376</td>\n",
       "      <td>-0.004443</td>\n",
       "      <td>0.051788</td>\n",
       "      <td>0.310443</td>\n",
       "      <td>0.310625</td>\n",
       "      <td>0.161061</td>\n",
       "      <td>0.436753</td>\n",
       "      <td>0.278871</td>\n",
       "      <td>1.707973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189964</td>\n",
       "      <td>0.175604</td>\n",
       "      <td>0.376266</td>\n",
       "      <td>0.050487</td>\n",
       "      <td>0.446751</td>\n",
       "      <td>0.359739</td>\n",
       "      <td>0.191530</td>\n",
       "      <td>0.175604</td>\n",
       "      <td>0.376266</td>\n",
       "      <td>0.050487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_seq_number  image_top_1     price  mobile_mean  mobile_std  \\\n",
       "0        -0.136324    -0.152807 -0.005147     0.376462    0.196650   \n",
       "1        -0.133284    -0.472375 -0.005101     0.380049    0.214786   \n",
       "2        -0.135073     1.894048 -0.005083     0.542563    0.095300   \n",
       "3        -0.085533    -0.367201 -0.005115     0.440850    0.415528   \n",
       "4        -0.136146     1.117376 -0.004443     0.051788    0.310443   \n",
       "\n",
       "   inception_mean  inception_std  nasnet_mean  nasnet_std  img_size_x  \\\n",
       "0        0.038229       0.360435    -0.071322    0.059919   -0.151260   \n",
       "1        0.588905       0.167156     0.240389   -0.033727   -0.138074   \n",
       "2        0.846048       0.241327     0.945804    0.401474    0.072903   \n",
       "3        0.831658       0.253381     0.486166    0.321294   -0.138074   \n",
       "4        0.310625       0.161061     0.436753    0.278871    1.707973   \n",
       "\n",
       "         ...         img_blur  img_blue_mean  img_green_mean  img_red_mean  \\\n",
       "0        ...        -0.345490      -0.280614       -0.638004      0.687033   \n",
       "1        ...         0.275917      -0.608886       -0.394327     -0.266605   \n",
       "2        ...        -0.254716       1.487169        1.379419      1.140900   \n",
       "3        ...        -0.372882       2.253272        2.131517      1.853883   \n",
       "4        ...        -0.189964       0.175604        0.376266      0.050487   \n",
       "\n",
       "   img_blue_std  img_green_std  img_red_std  img_average_red  \\\n",
       "0      0.641418       0.851439     0.132975        -0.280614   \n",
       "1      0.451465       0.240082     0.133307        -0.608886   \n",
       "2      1.906005       1.904254     1.820077         1.487169   \n",
       "3      0.683302       0.762878     0.720346         2.253272   \n",
       "4      0.446751       0.359739     0.191530         0.175604   \n",
       "\n",
       "   img_average_green  img_average_blue  \n",
       "0          -0.638004          0.687033  \n",
       "1          -0.394327         -0.266605  \n",
       "2           1.379419          1.140900  \n",
       "3           2.131517          1.853883  \n",
       "4           0.376266          0.050487  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Create item description feature:] done in 4 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Create item description feature:\"):\n",
    "    count = 0\n",
    "    desc_cat = ['category_name', 'param_1', 'param_2', 'param_3']\n",
    "    for c in desc_cat:\n",
    "        if count == 0:\n",
    "            train['item_desc_cat'] = train[c].astype(str)\n",
    "            count += 1\n",
    "        else:\n",
    "            train['item_desc_cat'] += '_' + train[c].astype(str)\n",
    "\n",
    "    count = 0\n",
    "    for c in desc_cat:\n",
    "        if count == 0:\n",
    "            test['item_desc_cat'] = test[c].astype(str)\n",
    "            count += 1\n",
    "        else:\n",
    "            test['item_desc_cat'] += '_' + test[c].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Create user description feature:] done in 3 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Create user description feature:\"):\n",
    "    count = 0\n",
    "    desc_cat = ['user_type', 'region', 'city']\n",
    "    for c in desc_cat:\n",
    "        if count == 0:\n",
    "            train['user_desc_cat'] = train[c].astype(str)\n",
    "            count += 1\n",
    "        else:\n",
    "            train['user_desc_cat'] += '_' + train[c].astype(str)\n",
    "\n",
    "    count = 0\n",
    "    for c in desc_cat:\n",
    "        if count == 0:\n",
    "            test['user_desc_cat'] = test[c].astype(str)\n",
    "            count += 1\n",
    "        else:\n",
    "            test['user_desc_cat'] += '_' + test[c].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Create categorical count feature:] done in 9 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Create categorical count feature:\"):\n",
    "    cat_count_features = []\n",
    "    for c in cat_cols + ['item_desc_cat'] + ['user_desc_cat']:\n",
    "        d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n",
    "        train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n",
    "        test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n",
    "        cat_count_features.append('%s_count'%c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_count</th>\n",
       "      <th>city_count</th>\n",
       "      <th>parent_category_name_count</th>\n",
       "      <th>category_name_count</th>\n",
       "      <th>user_type_count</th>\n",
       "      <th>param_1_count</th>\n",
       "      <th>param_2_count</th>\n",
       "      <th>param_3_count</th>\n",
       "      <th>price_missing_count</th>\n",
       "      <th>image_top_1_missing_count</th>\n",
       "      <th>item_desc_cat_count</th>\n",
       "      <th>user_desc_cat_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127883</td>\n",
       "      <td>85993</td>\n",
       "      <td>914200</td>\n",
       "      <td>135280</td>\n",
       "      <td>1433965</td>\n",
       "      <td>8634</td>\n",
       "      <td>887771</td>\n",
       "      <td>1168896</td>\n",
       "      <td>1895915</td>\n",
       "      <td>1856665</td>\n",
       "      <td>8634</td>\n",
       "      <td>61301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99914</td>\n",
       "      <td>57398</td>\n",
       "      <td>243733</td>\n",
       "      <td>87217</td>\n",
       "      <td>1433965</td>\n",
       "      <td>38156</td>\n",
       "      <td>887771</td>\n",
       "      <td>1168896</td>\n",
       "      <td>1895915</td>\n",
       "      <td>1856665</td>\n",
       "      <td>6298</td>\n",
       "      <td>39989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116667</td>\n",
       "      <td>68019</td>\n",
       "      <td>231290</td>\n",
       "      <td>36014</td>\n",
       "      <td>1433965</td>\n",
       "      <td>2876</td>\n",
       "      <td>887771</td>\n",
       "      <td>1168896</td>\n",
       "      <td>1895915</td>\n",
       "      <td>1856665</td>\n",
       "      <td>2876</td>\n",
       "      <td>40142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111319</td>\n",
       "      <td>21135</td>\n",
       "      <td>914200</td>\n",
       "      <td>135280</td>\n",
       "      <td>467198</td>\n",
       "      <td>6873</td>\n",
       "      <td>887771</td>\n",
       "      <td>1168896</td>\n",
       "      <td>1895915</td>\n",
       "      <td>1856665</td>\n",
       "      <td>6873</td>\n",
       "      <td>5609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64444</td>\n",
       "      <td>43920</td>\n",
       "      <td>109792</td>\n",
       "      <td>85101</td>\n",
       "      <td>1433965</td>\n",
       "      <td>83124</td>\n",
       "      <td>26463</td>\n",
       "      <td>2562</td>\n",
       "      <td>1895915</td>\n",
       "      <td>1856665</td>\n",
       "      <td>2562</td>\n",
       "      <td>29263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_count  city_count  parent_category_name_count  category_name_count  \\\n",
       "0        127883       85993                      914200               135280   \n",
       "1         99914       57398                      243733                87217   \n",
       "2        116667       68019                      231290                36014   \n",
       "3        111319       21135                      914200               135280   \n",
       "4         64444       43920                      109792                85101   \n",
       "\n",
       "   user_type_count  param_1_count  param_2_count  param_3_count  \\\n",
       "0          1433965           8634         887771        1168896   \n",
       "1          1433965          38156         887771        1168896   \n",
       "2          1433965           2876         887771        1168896   \n",
       "3           467198           6873         887771        1168896   \n",
       "4          1433965          83124          26463           2562   \n",
       "\n",
       "   price_missing_count  image_top_1_missing_count  item_desc_cat_count  \\\n",
       "0              1895915                    1856665                 8634   \n",
       "1              1895915                    1856665                 6298   \n",
       "2              1895915                    1856665                 2876   \n",
       "3              1895915                    1856665                 6873   \n",
       "4              1895915                    1856665                 2562   \n",
       "\n",
       "   user_desc_cat_count  \n",
       "0                61301  \n",
       "1                39989  \n",
       "2                40142  \n",
       "3                 5609  \n",
       "4                29263  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[cat_count_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [train_num.replace([np.inf, -np.inf, np.nan], 0), train[cat_count_features]]\n",
    "test_list = [test_num.replace([np.inf, -np.inf, np.nan], 0), test[cat_count_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Thomas\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Thomas\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "(1503424, 6) (508438, 6)\n",
      "[Create aggregated features:] done in 1143 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Create aggregated features:\"):\n",
    "    def proj_num_on_cat(train_df, test_df, target_column, group_column):\n",
    "        \"\"\"\n",
    "        :param train_df: train data frame\n",
    "        :param test_df:  test data frame\n",
    "        :param target_column: name of numerical feature\n",
    "        :param group_column: name of categorical feature\n",
    "        \"\"\"\n",
    "        train_df['row_id'] = range(train_df.shape[0])\n",
    "        test_df['row_id'] = range(test_df.shape[0])\n",
    "        train_df['train'] = 1\n",
    "        test_df['train'] = 0\n",
    "        all_df = train_df[['row_id', 'train', target_column, group_column]].append(test_df[['row_id','train',\n",
    "                                                                                            target_column, group_column]])\n",
    "        grouped = all_df[[target_column, group_column]].groupby(group_column)\n",
    "        the_size = pd.DataFrame(grouped.size()).reset_index()\n",
    "        the_size.columns = [group_column, '%s_size' % target_column]\n",
    "        the_mean = pd.DataFrame(grouped.mean()).reset_index()\n",
    "        the_mean.columns = [group_column, '%s_mean' % target_column]\n",
    "        the_std = pd.DataFrame(grouped.std()).reset_index().fillna(0)\n",
    "        the_std.columns = [group_column, '%s_std' % target_column]\n",
    "        the_median = pd.DataFrame(grouped.median()).reset_index()\n",
    "        the_median.columns = [group_column, '%s_median' % target_column]\n",
    "        the_stats = pd.merge(the_size, the_mean)\n",
    "        the_stats = pd.merge(the_stats, the_std)\n",
    "        the_stats = pd.merge(the_stats, the_median)\n",
    "\n",
    "        the_max = pd.DataFrame(grouped.max()).reset_index()\n",
    "        the_max.columns = [group_column, '%s_max' % target_column]\n",
    "        the_min = pd.DataFrame(grouped.min()).reset_index()\n",
    "        the_min.columns = [group_column, '%s_min' % target_column]\n",
    "\n",
    "        the_stats = pd.merge(the_stats, the_max)\n",
    "        the_stats = pd.merge(the_stats, the_min)\n",
    "\n",
    "        all_df = pd.merge(all_df, the_stats, how='left')\n",
    "\n",
    "        selected_train = all_df[all_df['train'] == 1]\n",
    "        selected_test = all_df[all_df['train'] == 0]\n",
    "        selected_train.sort_values('row_id', inplace=True)\n",
    "        selected_test.sort_values('row_id', inplace=True)\n",
    "        selected_train.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "        selected_test.drop([target_column, group_column, 'row_id', 'train'], axis=1, inplace=True)\n",
    "\n",
    "        selected_train, selected_test = np.array(selected_train), np.array(selected_test)\n",
    "        print(selected_train.shape, selected_test.shape)\n",
    "        return selected_train, selected_test\n",
    "    \n",
    "    for t in ['image_top_1','price', 'item_seq_number', 'mobile_mean', 'mobile_std', 'inception_mean',\n",
    "                      'inception_std', 'nasnet_mean', 'nasnet_std']:\n",
    "        for g in ['image_top_1','price', 'item_seq_number', 'mobile_mean', 'mobile_std', 'inception_mean',\n",
    "                  'inception_std', 'nasnet_mean', 'nasnet_std', 'city']:\n",
    "            if t != g:\n",
    "                s_train, s_test = proj_num_on_cat(train, test, target_column=t, group_column=g)\n",
    "                train_list.append(s_train)\n",
    "                test_list.append(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack(train_list).tocsr()\n",
    "X_test = hstack(test_list).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# model\n",
    "import lightgbm as lgb\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=23)\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)  \n",
    "dvalid = lgb.Dataset(X_valid, label=y_valid)  \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def input2params(max_depth=-1, num_leaves=31, min_data_in_leaf=20, lambda_l1=0, lambda_l2=0,\n",
    "                 feature_fraction=1.0, bagging_fraction=1.0): #scale_pos_weight=0.5,\n",
    "    \"\"\"parse BO input into an lgb friendly form\"\"\"\n",
    "  \n",
    "    def np2m1(x):\n",
    "        \"\"\"nearest power of two minus one\"\"\"\n",
    "        return max(int(round(2**np.round(np.log2(x)) - 1)), 1)\n",
    "    \n",
    "    return {\n",
    "        'objective': \"regression\",\n",
    "        'metric': 'rmse', \n",
    "        'boosting_type': \"gbdt\",\n",
    "        'learning_rate': 0.1,\n",
    "    \n",
    "        'num_leaves': np2m1(num_leaves),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "\n",
    "        'feature_fraction': np.clip(feature_fraction, 0.0, 1.0),\n",
    "        'bagging_fraction': np.clip(bagging_fraction, 0.0, 1.0),\n",
    "      \n",
    "        'lambda_l1': max(lambda_l1, 0),\n",
    "        'lambda_l2': max(lambda_l2, 0),   \n",
    "    \n",
    "#         'scale_pos_weight': int(scale_pos_weight),\n",
    "    }  \n",
    "\n",
    "def lgb_cv(dtrain, dvalid, **kwargs):\n",
    "    \n",
    "    params = input2params(**kwargs)\n",
    "\n",
    "    # fit model on training data    \n",
    "    bst = lgb.train(\n",
    "        params = params,\n",
    "        train_set = dtrain, \n",
    "        valid_sets = [dtrain, dvalid],\n",
    "        num_boost_round = 10000, \n",
    "        early_stopping_rounds = 25,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    bo_score = np.mean(bst.best_score['valid_1']['rmse']) * -1\n",
    "    \n",
    "#     f, ax = plt.subplots(figsize=[7,10])\n",
    "#     lgb.plot_importance(bst, max_num_features=50, ax=ax)\n",
    "#     plt.title(\"Light GBM Feature Importance\")\n",
    "#     plt.show() \n",
    "#     return np.mean(bst.best_score['valid_1']['rmse'])\n",
    "\n",
    "    return bo_score\n",
    "\n",
    "param_lims = {\n",
    "    'num_leaves': (128, 512),\n",
    "    'min_data_in_leaf': (15, 256),\n",
    "    'feature_fraction': (0.1, 1.0),\n",
    "    'bagging_fraction': (0.1, 1.0),\n",
    "    'lambda_l1': (0.0, 1.0),\n",
    "    'lambda_l2': (0.0, 1.0),\n",
    "#     'scale_pos_weight': (50, 450),\n",
    "}\n",
    "\n",
    "param_log = []\n",
    "\n",
    "lgb_opt = partial(lgb_cv, dtrain, dvalid)\n",
    "BO = BayesianOptimization(lgb_opt, param_lims)\n",
    "BO.maximize(init_points=5, n_iter=5)\n",
    "\n",
    "# param_log.append({**BO.res['max']['max_params']})    \n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
